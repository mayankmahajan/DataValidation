SET spark.sql.hive.version=0.13.1
SET spark.sql.parquet.filterPushdown=true
SET spark.sql.hive.convertMetastoreParquet=true
SET spark.sql.parquet.cacheMetadata=true
spark-sql> CREATE TEMPORARY FUNCTION DenseVectorUDF as 'com.guavus.densevectorudf.DenseVectorUDF'; CREATE TEMPORARY FUNCTION peakUDF as 'com.guavus.densevectorudf.PeakDenseVectorUDF'; CREATE TEMPORARY FUNCTION genericUDAF as 'com.guavus.densevectorudf.GenericDenseVectorUDAFResolver'; CREATE TEMPORARY FUNCTION averageUDF as 'com.guavus.densevectorudf.AverageDenseVectorUDF';use RC2_p71_db; select timestamp, nfnameid, isipv6, DenseVectorUDF(T.ByteBuffer), DenseVectorUDF(T.FlowBuffer), DenseVectorUDF(T.CostBuffer) from (select nfnameid, isipv6 , timestamp, genericUDAF(bytebuffer) as ByteBuffer, genericUDAF(flowbuffer) as FlowBuffer, genericUDAF(CostBuffer) as  CostBuffer from  f_nrmca_60min_3600_nfcube where timestamp=1451192400 group by nfnameid, isipv6, timestamp) T;
1451192400	5	0	[1.36000006E12,1.35000005E12,1.35000005E12,1.31000002E12,1.33000004E12,1.30000001E12,1.29000001E12,1.26999999E12,1.25999999E12,1.22999996E12,1.19999994E12,1.19999994E12]	[3.56E7,3.51E7,3.45E7,3.38E7,3.45E7,3.37E7,3.34E7,3.28E7,3.15E7,3.18E7,3.23E7,2.96E7]	[311.0]
1451192400	4	0	[7.56E9,7.2199997E9,7.2999997E9,7.12E9,7.0599997E9,6.9799997E9,6.92E9,6.8300001E9,6.6200003E9,6.6499999E9,6.5400003E9,6.48E9]	[4.8E7,4.6E7,4.7E7,4.56E7,4.54E7,4.51E7,4.46E7,4.4E7,4.28E7,4.33E7,4.31E7,4.21E7]	[2.16]
1451192400	10	0	[6.67E8,6.51E8,6.39E8,6.34E8,6.32E8,6.15E8,6.24E8,6.17E8,5.99E8,5.95E8,5.84E8,5.97E8]	[865000.0,809000.0,798000.0,753000.0,741000.0,720000.0,769000.0,744000.0,756000.0,729000.0,748000.0,721000.0]	[0.069]
1451192400	16	0	[3.68999989E11,3.68000008E11,3.56000006E11,3.54999992E11,3.45999999E11,3.35999992E11,3.26999998E11,3.16999991E11,3.07999998E11,3.02999994E11,3.04000008E11,2.87000003E11]	[1.95E7,1.93E7,1.9E7,1.87E7,1.81E7,1.83E7,1.87E7,1.8E7,1.78E7,1.74E7,1.83E7,1.71E7]	[36.8]
1451192400	22	0	[6.7199998E11,6.6699998E11,6.5799999E11,6.5000001E11,6.3100001E11,6.1499998E11,6.4E11,6.16E11,6.2999999E11,6.2300003E11,5.97E11,5.8699999E11]	[1.16E7,1.13E7,1.16E7,1.11E7,1.11E7,1.1E7,1.08E7,1.07E7,1.04E7,1.06E7,1.09E7,1.02E7]	[78.9]
1451192400	15	0	[7.7E7,6.34E7,5.8E7,7.58E7,6.48E7,7.17E7,6.97E7,6.95E7,6.16E7,6.25E7,6.42E7,6.68E7]	[36400.0,16400.0,23400.0,30400.0,30400.0,26400.0,28400.0,22400.0,22400.0,15400.0,32400.0,19400.0]	[0.0294]
1451192400	14	0	[4.2E8,4.23E8,4.09E8,4.4E8,4.04E8,3.75E8,3.79E8,3.75E8,3.88E8,3.88E8,3.87E8,3.89E8]	[30.5,29.0,29.0,29.5,31.5,29.0,25.5,35.5,29.5,29.0,26.5,30.5]	[0.0442]
1451192400	1	0	[9.53E8,9.45E8,9.49E8,8.94E8,9.09E8,8.83E8,8.8E8,8.56E8,8.74E8,8.68E8,8.64E8,8.12E8]	[230000.0,230000.0,228000.0,212000.0,216000.0,205000.0,203000.0,213000.0,205000.0,200000.0,197000.0,214000.0]	[0.099]
1451192400	6	0	[553000.0,614000.0,821000.0,1340000.0,492000.0,211000.0,418000.0,1970000.0,589000.0,899000.0,410000.0,951000.0]	[2.25,2.5,1.5,1003.0,2.0,1001.0,2.0,1003.0,2.25,1003.0,0.5,1002.0]	[8.57E-5]
1451192400	-1	0	[8.3199997E11,8.1899998E11,8.1799997E11,7.8300001E11,7.7099998E11,7.7199999E11,7.5799999E11,7.4199997E11,7.3600002E11,7.2600001E11,7.0400003E11,6.92E11]	[1.75E8,1.71E8,1.71E8,1.66E8,1.66E8,1.64E8,1.64E8,1.61E8,1.6E8,1.58E8,1.63E8,1.55E8]	[121.0]
spark-sql> Jan 14, 2016 5:35:02 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 10
Jan 14, 2016 5:35:02 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 10
Jan 14, 2016 5:35:02 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Jan 14, 2016 5:35:02 AM INFO: parquet.hadoop.ParquetFileReader: reading another 10 footers
Jan 14, 2016 5:35:02 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
