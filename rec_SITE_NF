SET spark.sql.hive.version=0.13.1
SET spark.sql.parquet.filterPushdown=true
SET spark.sql.hive.convertMetastoreParquet=true
SET spark.sql.parquet.cacheMetadata=true
spark-sql> CREATE TEMPORARY FUNCTION DenseVectorUDF as 'com.guavus.densevectorudf.DenseVectorUDF'; CREATE TEMPORARY FUNCTION peakUDF as 'com.guavus.densevectorudf.PeakDenseVectorUDF'; CREATE TEMPORARY FUNCTION genericUDAF as 'com.guavus.densevectorudf.GenericDenseVectorUDAFResolver'; CREATE TEMPORARY FUNCTION averageUDF as 'com.guavus.densevectorudf.AverageDenseVectorUDF';use RC2_p71_db; select sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, isipv6, timestamp, DenseVectorUDF(T.Downlinkbyte), DenseVectorUDF(T.upLinkByteBuffer), DenseVectorUDF(T.uplinkflowbuffer),  DenseVectorUDF(T.downlinkflowbuffer)  , DenseVectorUDF(T.uplinkcostbuffer), DenseVectorUDF(T.downlinkcostbuffer) from (select sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, isipv6 , timestamp, genericUDAF(downlinkbytebuffer) as Downlinkbyte, genericUDAF(uplinkbytebuffer) as  upLinkByteBuffer, genericUDAF(uplinkflowbuffer) as uplinkflowbuffer,  genericUDAF(downlinkflowbuffer) as downlinkflowbuffer , genericUDAF(uplinkcostbuffer) as  uplinkcostbuffer, genericUDAF(downlinkcostbuffer) as downlinkcostbuffer from  f_nrmca_60min_3600_siteflowdatacube where timestamp=1451001600 and sourcesiteid = 18 group by sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, isipv6, timestamp) T;
18	1	-1	-1	0	1451001600	[3.92999993E11,2.36000002E11,2.45999993E11,2.41000006E11,2.43999998E11,2.30999998E11,2.32999993E11,2.32999993E11,2.40000008E11,2.42000003E11,2.36000002E11,2.28000006E11]	[1.34000004E12,7.9400003E11,8.0800003E11,8.1399998E11,8.2299997E11,8.0200001E11,8.2899999E11,8.1300003E11,8.25E11,8.2700003E11,8.1799997E11,8.1399998E11]	[3.56E8,1.85E8,1.86E8,1.88E8,1.87E8,1.86E8,1.88E8,1.84E8,1.86E8,1.86E8,1.84E8,1.84E8]	[1.55E8,7.9E7,8.01E7,8.13E7,8.03E7,8.0E7,7.99E7,7.84E7,7.96E7,7.89E7,7.89E7,7.83E7]	[103.0]	[30.9]
18	1	-1	5	0	1451001600	[4.77000008E11,2.97999991E11,3.04999989E11,3.14999996E11,2.85999989E11,2.95999996E11,2.99000005E11,2.99000005E11,3.02999994E11,3.06000003E11,2.92000006E11,2.95999996E11]	[1.67000002E12,1.31000002E12,1.31000002E12,1.35000005E12,1.30000001E12,1.30000001E12,1.32000003E12,1.30000001E12,1.31000002E12,1.31000002E12,1.29000001E12,1.29000001E12]	[1.21E8,6.45E7,6.73E7,6.92E7,6.48E7,6.71E7,6.55E7,6.47E7,6.7E7,6.71E7,6.37E7,6.47E7]	[6.32E7,3.35E7,3.48E7,3.56E7,3.35E7,3.47E7,3.38E7,3.37E7,3.45E7,3.46E7,3.31E7,3.33E7]	[223.0]	[35.8]
spark-sql> Jan 12, 2016 11:14:10 AM INFO: parquet.filter2.compat.FilterCompat: Filtering using predicate: eq(sourcesiteid, 18)
Jan 12, 2016 11:14:10 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 113
Jan 12, 2016 11:14:10 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 113
Jan 12, 2016 11:14:10 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Jan 12, 2016 11:14:11 AM INFO: parquet.hadoop.ParquetFileReader: reading another 113 footers
Jan 12, 2016 11:14:11 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
