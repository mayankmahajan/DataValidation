SET spark.sql.hive.version=0.13.1
SET spark.sql.parquet.filterPushdown=true
SET spark.sql.hive.convertMetastoreParquet=true
SET spark.sql.parquet.cacheMetadata=true
spark-sql> CREATE TEMPORARY FUNCTION DenseVectorUDF as 'com.guavus.densevectorudf.DenseVectorUDF'; CREATE TEMPORARY FUNCTION peakUDF as 'com.guavus.densevectorudf.PeakDenseVectorUDF'; CREATE TEMPORARY FUNCTION genericUDAF as 'com.guavus.densevectorudf.GenericDenseVectorUDAFResolver'; CREATE TEMPORARY FUNCTION averageUDF as 'com.guavus.densevectorudf.AverageDenseVectorUDF';use RC2_p71_db; select timestamp, nfnameid, isipv6, DenseVectorUDF(T.ByteBuffer), DenseVectorUDF(T.FlowBuffer), DenseVectorUDF(T.CostBuffer) from (select nfnameid, isipv6 , timestamp, genericUDAF(bytebuffer) as ByteBuffer, genericUDAF(flowbuffer) as FlowBuffer, genericUDAF(CostBuffer) as  CostBuffer from  f_nrmca_60min_3600_nfcube where timestamp=1451329200 group by nfnameid, isipv6, timestamp) T;
1451329200	5	0	[1.46E12,1.44999999E12,1.44999999E12,1.48000001E12,1.46E12,1.46E12,1.44999999E12,1.44999999E12,1.46E12,1.47000001E12,1.46E12,1.42999998E12]	[4.11E7,4.12E7,4.04E7,4.18E7,4.07E7,4.16E7,4.27E7,4.17E7,4.09E7,4.11E7,3.99E7,3.99E7]	[350.0]
1451329200	4	0	[1.08E10,1.10000005E10,1.08999997E10,1.08999997E10,1.08E10,1.08E10,1.10000005E10,1.08999997E10,1.08E10,1.08E10,1.07000003E10,1.08999997E10]	[6.36E7,6.5E7,6.43E7,6.47E7,6.36E7,6.4E7,6.48E7,6.46E7,6.36E7,6.34E7,6.32E7,6.42E7]	[3.38]
1451329200	10	0	[9.13E8,9.18E8,8.99E8,9.2E8,9.03E8,9.08E8,9.11E8,9.06E8,8.88E8,9.07E8,8.9E8,8.99E8]	[1080000.0,1090000.0,1070000.0,1080000.0,1030000.0,1040000.0,1080000.0,1090000.0,1030000.0,1090000.0,1080000.0,1050000.0]	[0.1006]
1451329200	16	0	[4.48E11,4.34999984E11,4.49000014E11,4.51000009E11,4.37000012E11,4.46999986E11,4.54999998E11,4.49999995E11,4.53000004E11,4.39999988E11,4.44999991E11,4.32000008E11]	[2.54E7,2.58E7,2.51E7,2.54E7,2.45E7,2.43E7,2.53E7,2.51E7,2.48E7,2.48E7,2.49E7,2.45E7]	[49.5]
1451329200	22	0	[6.9900003E11,6.9999998E11,7.0099999E11,7.0300002E11,7.3100001E11,6.9599999E11,6.4600002E11,6.8000003E11,7.0599999E11,6.9599999E11,6.9300001E11,6.92E11]	[1.34E7,1.3E7,1.32E7,1.34E7,1.31E7,1.32E7,1.28E7,1.31E7,1.34E7,1.3E7,1.3E7,1.27E7]	[84.0]
1451329200	15	0	[1.03E8,9.17E7,8.35E7,9.11E7,9.31E7,9.39E7,9.1E7,9.49E7,8.7E7,9.61E7,8.86E7,9.15E7]	[25500.0,22500.0,28500.0,22500.0,17500.0,25500.0,28500.0,17500.0,20500.0,19500.0,23500.0,22500.0]	[0.0405]
1451329200	14	0	[4.34E8,4.18E8,4.21E8,4.28E8,4.1E8,4.21E8,4.26E8,4.2E8,3.9E8,4.32E8,4.26E8,4.22E8]	[28.5,29.0,28.0,37.0,27.0,32.0,30.5,31.5,27.5,29.0,32.0,28.0]	[0.0467]
1451329200	1	0	[1.17E9,1.15E9,1.14E9,1.16E9,1.16E9,1.15E9,1.16E9,1.16E9,1.13E9,1.15E9,1.15E9,1.14E9]	[260000.0,252000.0,252000.0,241000.0,248000.0,244000.0,251000.0,232000.0,225000.0,271000.0,223000.0,235000.0]	[0.128]
1451329200	6	0	[364000.0,67000.0,296000.0,683000.0,1140000.0,1160000.0,23000.0,522000.0,46000.0,1250000.0,990000.0,84500.0]	[1.75,1.5,1.75,0.75,2.25,3.0,0.5,3.0,1.0,1003.0,2.5,1.0]	[6.14E-5]
1451329200	-1	0	[1.05999997E12,1.05999997E12,1.04000002E12,1.04000002E12,1.03000002E12,1.02100002E12,1.04000002E12,1.03000002E12,1.03000002E12,1.05000003E12,1.04000002E12,1.03000002E12]	[2.33E8,2.31E8,2.3E8,2.32E8,2.26E8,2.28E8,2.32E8,2.32E8,2.29E8,2.31E8,2.28E8,2.28E8]	[178.0]
spark-sql> Jan 15, 2016 9:21:04 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 10
Jan 15, 2016 9:21:04 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 10
Jan 15, 2016 9:21:04 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Jan 15, 2016 9:21:04 AM INFO: parquet.hadoop.ParquetFileReader: reading another 10 footers
Jan 15, 2016 9:21:04 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
