SET spark.sql.hive.version=0.13.1
SET spark.sql.parquet.filterPushdown=true
SET spark.sql.hive.convertMetastoreParquet=true
SET spark.sql.parquet.cacheMetadata=true
spark-sql> CREATE TEMPORARY FUNCTION DenseVectorUDF as 'com.guavus.densevectorudf.DenseVectorUDF'; CREATE TEMPORARY FUNCTION peakUDF as 'com.guavus.densevectorudf.PeakDenseVectorUDF'; CREATE TEMPORARY FUNCTION genericUDAF as 'com.guavus.densevectorudf.GenericDenseVectorUDAFResolver'; CREATE TEMPORARY FUNCTION averageUDF as 'com.guavus.densevectorudf.AverageDenseVectorUDF';use RC2_p71_db; select sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, destsiteid, destsitetypeid, destsiteelementtypeid, isipv6, vrfid , timestamp, DenseVectorUDF(Downlinkbyte), DenseVectorUDF(uplinkbytebuffer), DenseVectorUDF(uplinkflowbuffer),  DenseVectorUDF(downlinkflowbuffer)  , DenseVectorUDF(uplinkcostbuffer), DenseVectorUDF(downlinkcostbuffer)from (select sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, destsiteid, destsitetypeid, destsiteelementtypeid, isipv6, vrfid , timestamp, genericUDAF(downlinkbytebuffer) as Downlinkbyte, genericUDAF(uplinkbytebuffer) as  upLinkByteBuffer, genericUDAF(uplinkflowbuffer) as uplinkflowbuffer,  genericUDAF(downlinkflowbuffer) as downlinkflowbuffer , genericUDAF(uplinkcostbuffer) as  uplinkcostbuffer, genericUDAF(downlinkcostbuffer) as downlinkcostbuffer from  f_nrmca_60min_3600_siteflowdatacube where timestamp=1451192400 and destsiteid=27 and sourcesiteid=18 and sourcesitetypeid=1 and destsitetypeid=1   group by sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, destsiteid, destsitetypeid, destsiteelementtypeid, isipv6, vrfid , timestamp) T;
18	1	-1	-1	27	1	-1	0	5	1451192400	[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]	[9880000.0,6710000.0,6240000.0,2190000.0,5940000.0,2880000.0,1440000.0,1140000.0,4210000.0,1800000.0,683000.0,2690000.0]	[10.5,10.5,8.5,7.5,8.5,4.0,1.5,3.0,7.0,5.0,1.5,5.5]	[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]	[4.24E-4]	[0.0]
18	1	-1	22	27	1	-1	0	1	1451192400	[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]	[5.79E7,5.6E7,4.9E7,1.78E8,7.39E7,4.75E7,6.21E7,6.4E7,7.09E7,5.68E7,5.3E7,4.83E7]	[154.0,149.0,139.0,139.0,145.0,140.0,152.0,143.0,143.0,135.0,125.0,114.0]	[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]	[0.00757]	[0.0]
18	1	-1	1	27	1	-1	0	2	1451192400	[0.0,0.0,45000.0,0.0,45000.0,0.0,42000.0,0.0,0.0,0.0,0.0,0.0]	[31000.0,0.0,13000.0,0.0,0.0,13000.0,0.0,0.0,62000.0,34000.0,0.0,0.0]	[0.25,0.0,0.25,0.0,0.0,0.25,0.0,0.0,0.5,0.25,0.0,0.0]	[0.0,0.0,0.25,0.0,0.25,0.0,0.25,0.0,0.0,0.0,0.0,0.0]	[1.42E-6]	[1.22E-6]
18	1	-1	10	27	1	-1	0	2	1451192400	[2.01E7,1.79E7,1.93E7,1.96E7,1.89E7,1.78E7,1.89E7,1.86E7,1.74E7,1.71E7,1.67E7,1.67E7]	[6.76E7,6.51E7,6.73E7,6.67E7,6.44E7,6.66E7,6.47E7,5.85E7,6.04E7,6.03E7,5.51E7,6.05E7]	[236.0,230.0,235.0,247.0,236.0,238.0,244.0,237.0,237.0,244.0,244.0,244.0]	[113000.0,92000.0,104000.0,108000.0,87600.0,94000.0,95000.0,93000.0,95500.0,80500.0,83000.0,94500.0]	[0.00701]	[0.00203]
18	1	-1	4	27	1	-1	0	2	1451192400	[4.3E8,4.21E8,4.28E8,4.2E8,4.26E8,4.18E8,4.17E8,4.05E8,4.09E8,3.93E8,3.96E8,3.97E8]	[4.81E8,4.81E8,4.85E8,4.72E8,4.73E8,4.71E8,4.57E8,4.42E8,4.47E8,4.27E8,4.28E8,4.3E8]	[4280.0,4290.0,4270.0,4170.0,4140.0,4140.0,4070.0,4000.0,3990.0,3840.0,3860.0,3810.0]	[4490.0,4440.0,4390.0,4360.0,4420.0,4290.0,4290.0,4160.0,4220.0,4030.0,4110.0,4050.0]	[0.0509]	[0.0459]
18	1	-1	22	27	1	-1	0	2	1451192400	[6.24E8,6.31E8,5.65E8,6.11E8,5.82E8,5.73E8,5.88E8,6.11E8,6.06E8,5.39E8,5.52E8,5.18E8]	[9.16E8,1.11E9,1.007E9,9.79E8,1.04E9,7.92E8,1.04E9,1.14E9,9.14E8,7.73E8,7.79E8,7.94E8]	[3270.0,756.0,2260.0,1760.0,4280.0,5270.0,4270.0,1760.0,1730.0,2240.0,4240.0,2750.0]	[64000.0,69000.0,66000.0,77500.0,64500.0,61500.0,63000.0,73500.0,68500.0,62500.0,70000.0,59000.0]	[0.104]	[0.0648]
18	1	-1	5	27	1	-1	0	2	1451192400	[3.4E8,3.35E8,2.55E8,2.21E8,2.37E8,2.66E8,2.38E8,2.46E8,2.21E8,2.37E8,2.61E8,2.68E8]	[2.94E9,3.0E9,2.79000013E9,2.59000013E9,2.91000013E9,3.1E9,2.94E9,2.80999987E9,2.64999987E9,2.86E9,2.99000013E9,2.84E9]	[714.0,703.0,678.0,673.0,647.0,698.0,681.0,657.0,616.0,644.0,643.0,663.0]	[157.0,148.0,145.0,141.0,133.0,143.0,135.0,133.0,149.0,144.0,133.0,143.0]	[0.319]	[0.0289]
18	1	-1	-1	27	1	-1	0	7	1451192400	[4.31E7,4.63E7,5.77E7,4.16E7,4.47E7,4.58E7,4.62E7,3.41E7,4.94E7,4.62E7,3.83E7,4.48E7]	[2.15E7,2.27E7,2.34E7,2.24E7,2.75E7,2.7E7,3.29E7,2.1E7,3.03E7,1.67E7,1.66E7,2.58E7]	[41.5,42.0,39.0,40.0,39.5,39.0,42.5,41.0,47.0,41.0,42.5,37.5]	[37000.0,39000.0,48000.0,34000.0,37000.0,39000.0,38000.0,28000.0,42000.0,37000.0,30000.0,37000.0]	[0.00267]	[0.00498]
18	1	-1	16	27	1	-1	0	2	1451192400	[1.43E9,1.17E9,1.07E9,1.27E9,1.15E9,1.15E9,1.21E9,1.16E9,1.12E9,1.09E9,1.12E9,8.51E8]	[1.01499996E10,1.12999997E10,1.08E10,1.01400003E10,1.12E10,9.9300004E9,9.68E9,1.00699996E10,9.9000003E9,1.016E10,1.11000003E10,9.7799997E9]	[291000.0,298000.0,288000.0,265000.0,274000.0,266000.0,279000.0,281000.0,277000.0,275000.0,326000.0,263000.0]	[73500.0,78500.0,74500.0,70000.0,71000.0,74500.0,71500.0,76500.0,74000.0,72000.0,78000.0,73000.0]	[1.15]	[0.128]
18	1	-1	-1	27	1	-1	0	2	1451192400	[2.79000013E9,2.74E9,2.82E9,2.72E9,2.71000013E9,2.7E9,2.68E9,2.63000013E9,2.55000013E9,2.64E9,2.56E9,2.59000013E9]	[2.10999992E10,2.06000005E10,2.2000001E10,2.08E10,2.13000008E10,2.08E10,1.98000005E10,1.98000005E10,1.97000008E10,2.00999997E10,1.92999997E10,1.93999995E10]	[69500.0,70500.0,69500.0,70000.0,72000.0,66500.0,63000.0,71500.0,76500.0,66400.0,83000.0,62500.0]	[386000.0,358000.0,388000.0,381000.0,363000.0,350000.0,354000.0,342000.0,337000.0,341000.0,384000.0,362000.0]	[2.27]	[0.298]
spark-sql> Jan 14, 2016 6:41:10 AM INFO: parquet.filter2.compat.FilterCompat: Filtering using predicate: and(and(and(eq(destsiteid, 27), eq(sourcesiteid, 18)), eq(sourcesitetypeid, 1)), eq(destsitetypeid, 1))
Jan 14, 2016 6:41:10 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 121
Jan 14, 2016 6:41:10 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 121
Jan 14, 2016 6:41:10 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Jan 14, 2016 6:41:10 AM INFO: parquet.hadoop.ParquetFileReader: reading another 121 footers
Jan 14, 2016 6:41:10 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
