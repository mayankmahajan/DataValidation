SET spark.sql.hive.version=0.13.1
SET spark.sql.parquet.filterPushdown=true
SET spark.sql.hive.convertMetastoreParquet=true
SET spark.sql.parquet.cacheMetadata=true
spark-sql> CREATE TEMPORARY FUNCTION DenseVectorUDF as 'com.guavus.densevectorudf.DenseVectorUDF'; CREATE TEMPORARY FUNCTION peakUDF as 'com.guavus.densevectorudf.PeakDenseVectorUDF'; CREATE TEMPORARY FUNCTION genericUDAF as 'com.guavus.densevectorudf.GenericDenseVectorUDAFResolver'; CREATE TEMPORARY FUNCTION averageUDF as 'com.guavus.densevectorudf.AverageDenseVectorUDF';use rc2p61db_temp; select sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, isipv6, timestamp, DenseVectorUDF(T.Downlinkbyte), DenseVectorUDF(T.upLinkByteBuffer), DenseVectorUDF(T.uplinkflowbuffer),  DenseVectorUDF(T.downlinkflowbuffer)  , DenseVectorUDF(T.uplinkcostbuffer), DenseVectorUDF(T.downlinkcostbuffer) from (select sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, isipv6 , timestamp, genericUDAF(downlinkbytebuffer) as Downlinkbyte, genericUDAF(uplinkbytebuffer) as  upLinkByteBuffer, genericUDAF(uplinkflowbuffer) as uplinkflowbuffer,  genericUDAF(downlinkflowbuffer) as downlinkflowbuffer , genericUDAF(uplinkcostbuffer) as  uplinkcostbuffer, genericUDAF(downlinkcostbuffer) as downlinkcostbuffer from  f_nrmca_60min_3600_sitedatacube where timestamp=1450915200 and sourcesiteid=18 and nfnameid=4 group by sourcesiteid, sourcesitetypeid, sourcesiteelementid, nfnameid, isipv6, timestamp) T;
18	1	-1	4	0	1450915200	[5.0099999E9,4.96E9,4.8600003E9,4.8999997E9,4.9299999E9,4.8300001E9,4.8199997E9,4.7900001E9,4.8E9,4.7100001E9,4.7000003E9,4.6200003E9]	[5.2700001E9,5.2199997E9,5.1100001E9,5.12E9,5.1800003E9,5.0700001E9,5.0700001E9,5.04E9,5.0700001E9,4.9699999E9,4.9500001E9,4.8499999E9]	[3.18E7,3.13E7,3.06E7,3.1E7,3.14E7,3.07E7,3.02E7,3.06E7,3.11E7,3.02E7,3.01E7,2.95E7]	[3.34E7,3.3E7,3.23E7,3.27E7,3.31E7,3.24E7,3.19E7,3.22E7,3.27E7,3.19E7,3.17E7,3.12E7]	[0.369]	[0.358]
spark-sql> Jan 8, 2016 8:30:41 AM INFO: parquet.filter2.compat.FilterCompat: Filtering using predicate: and(eq(sourcesiteid, 18), eq(nfnameid, 4))
Jan 8, 2016 8:30:42 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 65
Jan 8, 2016 8:30:42 AM INFO: parquet.hadoop.ParquetInputFormat: Total input paths to process : 65
Jan 8, 2016 8:30:42 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
Jan 8, 2016 8:30:42 AM INFO: parquet.hadoop.ParquetFileReader: reading another 65 footers
Jan 8, 2016 8:30:42 AM INFO: parquet.hadoop.ParquetFileReader: Initiating action with parallelism: 5
